<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0111)http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex4/ex4.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> 
	 
	 
	<title>
	Machine Learning	</title>
	<meta name="author" content="openclassroom.stanford.edu"> 
	<meta name="description" content="K-12 Free Education."> 
	
	<link rel="stylesheet" type="text/css" href="./Machine Learning_files/common.css">
	<!--<base href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex4/ex4.html">--><base href=".">


<script type="text/javascript" language="javascript">
var display = 0;
function showSolution()
  {
  if(display==0)
    {
    display = 1;
    document.getElementById("solutionDiv").style.display = "block";
    document.getElementById("button1").value = "Hide Solution";
    }
  else
    {
    display = 0;
    document.getElementById("solutionDiv").style.display = "none";
    document.getElementById("button1").value = "Show Solution";
    }
  }
</script>



	
</head> 
 
 	<body text="#000000" bgcolor="#FFFFFF"><div class="inner"> 
    
	<div id="header"> 			
		<ul id="navigation"> 
			<li><a href="http://openclassroom.stanford.edu/MainFolder/HomePage.php">OpenClassroom</a></li> 
		</ul> 
	</div> 
 
	<div id="main" class=""> 
		<div id="feature-bar">
			<div class="tabbed">
				<div id="left">
				<a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">
				    <img src="./Machine Learning_files/ML-icon2.png" alt="Machine" learning="" class="thumb"> 
				</a>
				</div>
   	    			<div id="title-text">
            			<a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning"><h1>Machine Learning</h1></a>
					<h2>Andrew Ng</h2>
            		</div>
			</div> 
    		</div>
		<div class="tabbed">
			<div class="results-list"> 
				

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->


<title>ex4</title>
<meta name="description" content="ex4">
<meta name="keywords" content="ex4">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">

<meta name="Generator" content="LaTeX2HTML v2008">
<meta http-equiv="Content-Style-Type" content="text/css">

<link rel="STYLESHEET" href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex4/ex4.css">





<p>

<big class="XHUGE"><b>Exercise 4: Logistic Regression and Newton's Method</b></big>

</p><p>
In this exercise, you will use Newton's Method to implement logistic regression 
on a classification problem.

</p><p>
<b>Data</b>

</p><p>
To begin, download 
<a href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex4materials/ex4Data.zip">ex4Data.zip</a> and extract the files from the zip file. 

</p><p>
For this exercise, suppose that a high school has a dataset representing 40 
students who were admitted to college and 40 students who were not
admitted.  Each&nbsp;<!-- MATH
 $(x^{(i)}, y^{(i)})$
 -->
<span class="MATH"><img width="80" height="41" align="MIDDLE" border="0" src="./Machine Learning_files/img1.png" alt="$(x^{(i)}, y^{(i)})$"></span> training example contains a student's score 
on two standardized exams and a label of whether the student was admitted.

</p><p>
Your task is to build a binary classification model that estimates college
admission chances based on a student's scores on two exams. In your training 
data,

</p><p>
<b>a. </b> The first column of your x array represents all Test 1 scores, 
and the second column represents all Test 2 scores.

</p><p>
<b>b. </b> The y vector uses '1' to label a student who was admitted and '0' to 
label a student who was not admitted.

</p><p>

</p><p></p><p>
<br>

</p><p>
<big class="XLARGE"><b>Plot the data</b></big> 

</p><p>
Load the data for the training examples into your program and add 
the <span class="MATH"><img width="57" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img2.png" alt="$x_0 = 1$"></span> intercept term into your x matrix. 

</p><p>
Before beginning Newton's Method, we will first plot the data using 
different symbols to represent the two classes. In Matlab/Octave,
 you can separate
the positive class and the negative class using the find command:

</p><p>
</p><pre>% find returns the indices of the
% rows meeting the specified condition
pos = find(y == 1); neg = find(y == 0);

% Assume the features are in the 2nd and 3rd
% columns of x
plot(x(pos, 2), x(pos,3), '+'); hold on
plot(x(neg, 2), x(neg, 3), 'o')
</pre>  

<p>
Your plot should look like the following:
 </p><div align="center"> 
<img src="./Machine Learning_files/ex4dataonly.png" alt="ex4dataonly" width="560"> </div> 

<p>
<big class="XLARGE"><b>Newton's Method</b></big> 

</p><p>
Recall that in logistic regression, the hypothesis function is
<br>
</p><div align="CENTER">

<!-- MATH
 \begin{eqnarray}
h_{\theta}(x)&=&g(\theta^{T}x)=\frac{{1}}{1+e^{-\theta^{T}x}} \nonumber \\
&=&P(y=1|x;\theta) \nonumber
\end{eqnarray}
 -->
<table cellpadding="0" align="CENTER" width="100%">
<tbody><tr valign="MIDDLE"><td nowrap="" width="50%" align="RIGHT"><img width="48" height="37" align="MIDDLE" border="0" src="./Machine Learning_files/img3.png" alt="$\displaystyle h_{\theta}(x)$"></td>
<td align="CENTER" nowrap=""><img width="19" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img4.png" alt="$\textstyle =$"></td>
<td align="LEFT" width="50%" nowrap=""><img width="164" height="58" align="MIDDLE" border="0" src="./Machine Learning_files/img5.png" alt="$\displaystyle g(\theta^{T}x)=\frac{{1}}{1+e^{-\theta^{T}x}}$"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
<tr valign="MIDDLE"><td nowrap="" width="50%" align="RIGHT">&nbsp;</td>
<td align="CENTER" nowrap=""><img width="19" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img4.png" alt="$\textstyle =$"></td>
<td align="LEFT" width="50%" nowrap=""><img width="112" height="37" align="MIDDLE" border="0" src="./Machine Learning_files/img6.png" alt="$\displaystyle P(y=1\vert x;\theta)$"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table></div>
<br clear="ALL"><p></p>

<p>
In our example, the hypothesis is interpreted as the
probability that a driver
will be accident-free, given the values of the features in x.

</p><p>
Matlab/Octave does not have a library function for the sigmoid, so you will have to
define it yourself. The easiest way to do this is through an inline expression:

</p><p>
</p><pre>g = inline('1.0 ./ (1.0 + exp(-z))'); 
% Usage: To find the value of the sigmoid 
% evaluated at 2, call g(2)
</pre>

<p>
The cost function <span class="MATH"><img width="41" height="37" align="MIDDLE" border="0" src="./Machine Learning_files/img7.png" alt="$J(\theta)$"></span> is defined as
<br>
</p><div align="RIGHT" class="mathdisplay">

<!-- MATH
 \begin{equation}
J(\theta)=\frac{{1}}{m}\sum_{i=1}^{m}\left[
 -y^{(i)}\log(h_{\theta}(x^{(i)}))-
 (1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))\right] 
\par
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="493" height="55" border="0" src="./Machine Learning_files/img8.png" alt="\begin{displaymath}
J(\theta)=\frac{{1}}{m}\sum_{i=1}^{m}\left[
-y^{(i)}\log(h_...
...
(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))\right] \nonumber
\par
\end{displaymath}"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>

<p>
Our goal is to use Newton's method to minimize this function. Recall that the 
update rule for Newton's method is
<br>
</p><div align="RIGHT" class="mathdisplay">

<!-- MATH
 \begin{equation}
\theta^{(t+1)}=\theta^{(t)}-H^{-1}\nabla_{\theta}J
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="186" height="31" border="0" src="./Machine Learning_files/img9.png" alt="\begin{displaymath}
\theta^{(t+1)}=\theta^{(t)}-H^{-1}\nabla_{\theta}J \nonumber
\end{displaymath}"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>

<p>
In logistic regression, the gradient and the Hessian are
<br>
</p><div align="RIGHT" class="mathdisplay">

<!-- MATH
 \begin{equation}
\nabla_{\theta}J = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="254" height="55" border="0" src="./Machine Learning_files/img10.png" alt="\begin{displaymath}
\nabla_{\theta}J = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)} \nonumber \nonumber
\end{displaymath}"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>
<br>
<div align="RIGHT" class="mathdisplay">

<!-- MATH
 \begin{equation}
H & = & \frac{1}{m}\sum_{i=1}^{m}\left[h_{\theta}(x^{(i)})\left(1-h_{\theta}(x^{(i)})\right)x^{(i)}\left(x^{(i)}\right)^{T}\right]
\end{equation}
 -->
<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="366" height="55" border="0" src="./Machine Learning_files/img11.png" alt="\begin{displaymath}
H &amp; = &amp; \frac{1}{m}\sum_{i=1}^{m}\left[h_{\theta}(x^{(i)})\l...
...^{(i)})\right)x^{(i)}\left(x^{(i)}\right)^{T}\right] \nonumber
\end{displaymath}"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table>
<br clear="ALL"></div><p></p>

<p>
Note that the formulas presented above are the vectorized versions. 
Specifically, this means that <!-- MATH
 $x^{(i)} \in R^{n+1}$
 -->
<span class="MATH"><img width="95" height="41" align="MIDDLE" border="0" src="./Machine Learning_files/img12.png" alt="$x^{(i)} \in R^{n+1}$"></span>, 
<!-- MATH
 $x^{(i)}\left(x^{(i)}\right)^{T}\in R^{(n+1) \times (n+1)}$
 -->
<span class="MATH"><img width="212" height="53" align="MIDDLE" border="0" src="./Machine Learning_files/img13.png" alt="$ x^{(i)}\left(x^{(i)}\right)^{T}\in R^{(n+1) \times (n+1)}$"></span>, while <!-- MATH
 $h_{\theta}(x^{(i)})$
 -->
<span class="MATH"><img width="64" height="41" align="MIDDLE" border="0" src="./Machine Learning_files/img14.png" alt="$h_{\theta}(x^{(i)})$"></span> and <span class="MATH"><img width="31" height="41" align="MIDDLE" border="0" src="./Machine Learning_files/img15.png" alt="$y^{(i)}$"></span> are scalars. 

</p><p>
<b>Implementation</b>

</p><p>
Now, implement Newton's Method in your program, starting with the initial value
of <!-- MATH
 $\theta = \vec{0}$
 -->
<span class="MATH"><img width="49" height="21" align="BOTTOM" border="0" src="./Machine Learning_files/img16.png" alt="$\theta = \vec{0}$"></span>. To determine how many iterations
to use, calculate <span class="MATH"><img width="41" height="37" align="MIDDLE" border="0" src="./Machine Learning_files/img7.png" alt="$J(\theta)$"></span> for each iteration and plot your results as you
did in Exercise&nbsp;2. As mentioned in the lecture videos, Newton's method often 
converges in 5-15 iterations. If you find yourself using far more iterations,
you should check for errors in your implementation.

</p><p>
After convergence, use your values of theta to find the decision boundary in
the classification problem. The decision boundary is defined as the line where
<br></p><p></p>
<div align="CENTER" class="mathdisplay">
<!-- MATH
 \begin{displaymath}
P(y=1|x; \theta) = g(\theta^T x) = 0.5
\end{displaymath}
 -->

<img width="234" height="33" border="0" src="./Machine Learning_files/img17.png" alt="\begin{displaymath}
P(y=1\vert x; \theta) = g(\theta^T x) = 0.5
\end{displaymath}">
</div>
<br clear="ALL">
<p></p>
which corresponds to 
<br><p></p>
<div align="CENTER" class="mathdisplay">
<!-- MATH
 \begin{displaymath}
\theta^T x = 0
\end{displaymath}
 -->

<img width="64" height="28" border="0" src="./Machine Learning_files/img18.png" alt="\begin{displaymath}
\theta^T x = 0
\end{displaymath}">
</div>
<br clear="ALL">
<p></p>

<p>
Plotting the decision boundary is equivalent to plotting
 the <!-- MATH
 $\theta^T x = 0$
 -->
<span class="MATH"><img width="70" height="19" align="BOTTOM" border="0" src="./Machine Learning_files/img19.png" alt="$\theta^T x = 0$"></span> line. When
you are finished, your plot should appear like the figure below.

</p><p>
 </p><div align="center"> 
<img src="./Machine Learning_files/ex4regression.png" alt="ex4regression" width="560"> </div> 

<p>
<b>Questions</b>

</p><p>
Finally, record your answers to these questions.

</p><p>
<b>1.</b> What values of <span class="MATH"><img width="14" height="17" align="BOTTOM" border="0" src="./Machine Learning_files/img20.png" alt="$\theta$"></span> did you get? How many iterations were 
required for convergence?

</p><p>
<b>2.</b> What is the probability that a student with a score of 20 on 
Exam&nbsp;1 and a score of 80 on Exam&nbsp;2 will not be admitted?

</p><p>

</p><div style="padding-left: 0px;">
<input id="button1" type="button" onclick="showSolution()" value="Show Solution" style="font-size: 20px;">
</div>

<div id="solutionDiv" style="display: none">

<p>
<big class="XHUGE"><b>Solutions</b></big> 

</p><p>
After you have completed the exercises above, please refer to the solutions below and check that your implementation and your 
answers are correct.  In a case where your implementation does not result in the same parameters/phenomena as described 
below, debug your solution until you manage to replicate the same effect as our implementation. 

</p><p>
A complete m-file implementation of the solutions can be found 
<a href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex4materials/ex4.m" target="_blank">here</a>.

</p><p>

<br><big class="XLARGE"><b>Newton's Method</b></big>

</p><p>
<b>1.</b> Your final values of theta should be
<br>
</p><div align="CENTER">

<!-- MATH
 \begin{eqnarray}
\theta_0 &=&  -16.38 \nonumber \\
  \theta_1 &=&   0.1483 \nonumber \\
  \theta_2 &=&  0.1589 \nonumber
\end{eqnarray}
 -->
<table cellpadding="0" align="CENTER" width="100%">
<tbody><tr valign="MIDDLE"><td nowrap="" width="50%" align="RIGHT"><img width="21" height="34" align="MIDDLE" border="0" src="./Machine Learning_files/img21.png" alt="$\displaystyle \theta_0$"></td>
<td align="CENTER" nowrap=""><img width="19" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img4.png" alt="$\textstyle =$"></td>
<td align="LEFT" width="50%" nowrap=""><img width="62" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img22.png" alt="$\displaystyle -16.38$"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
<tr valign="MIDDLE"><td nowrap="" width="50%" align="RIGHT"><img width="21" height="34" align="MIDDLE" border="0" src="./Machine Learning_files/img23.png" alt="$\displaystyle \theta_1$"></td>
<td align="CENTER" nowrap=""><img width="19" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img4.png" alt="$\textstyle =$"></td>
<td align="LEFT" width="50%" nowrap=""><img width="57" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img24.png" alt="$\displaystyle 0.1483$"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
<tr valign="MIDDLE"><td nowrap="" width="50%" align="RIGHT"><img width="21" height="34" align="MIDDLE" border="0" src="./Machine Learning_files/img25.png" alt="$\displaystyle \theta_2$"></td>
<td align="CENTER" nowrap=""><img width="19" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img4.png" alt="$\textstyle =$"></td>
<td align="LEFT" width="50%" nowrap=""><img width="57" height="33" align="MIDDLE" border="0" src="./Machine Learning_files/img26.png" alt="$\displaystyle 0.1589$"></td>
<td class="eqno" width="10" align="RIGHT">
&nbsp;</td></tr>
</tbody></table></div>
<br clear="ALL"><p></p>

<p>
<b>Plot. </b> Your plot of the cost function should look similar to the picture below:

</p><p>
 </p><div align="center"> 
<img src="./Machine Learning_files/ex4j.png" alt="ex4j" width="560"> </div> 

<p>
From this plot, you can infer that Newton's Method has converged by around 5 iterations.
In fact, by looking at a printout of the values of J, you will see that J changes
by less than <span class="MATH"><img width="41" height="19" align="BOTTOM" border="0" src="./Machine Learning_files/img27.png" alt="$10^{-7}$"></span> between the 4th and 5th
iterations. Recall that in the previous two exercises, gradient descent took
hundreds or even thousands of iterations to converge. Newton's Method is much
faster in comparison. 

</p><p>
<b>2.</b> The probability that a student with a score of 20 on Exam&nbsp;1 and 80 on
Exam&nbsp;2 will not be admitted to college is 0.668.

</p><p>
</p></div>

<p>
<br></p><hr>



			</div> 
 
			<div class="results-more"> 
				<h3>Resources</h3> <br>
				<ul class="related-resources"> 

<li><a href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/syllabus.html">Syllabus</a> </li><li><a href="http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/faq.html">FAQ</a> </li><li><a href="http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=notes/credits.html">Credits/Acknowledgments</a> </li>			

			</ul></div> 
		</div>
	</div> 
		
	<div id="footer"> 
	<div class="tabbed">
		<!--<p> 
			<a href="/about" rel="nofollow">About Us</a>&nbsp;&nbsp;|&nbsp;&nbsp;
			<a href="/pages/contact/" rel="nofollow">Contact</a>&nbsp;&nbsp;|&nbsp;&nbsp;
			<a href="/pages/privacy" rel="nofollow">Privacy Policy</a>&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;
			<a href="/pages/faq" rel="nofollow">FAQ</a>&nbsp;&nbsp;|&nbsp;&nbsp;</p> -->
			<p class="copyright">©  2010-2012 <!--<a href="HomePage.php" title="OpenClassroom" style="color: #666666; font-weight: normal; text-decoration: none;">OpenClassroom</a>-->Andrew Ng, Stanford University. All rights reserved.<br>

<!--
				<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a>
				This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a> !-->
			</p> 
  	</div>
</div>


 
 
</div></body></html>